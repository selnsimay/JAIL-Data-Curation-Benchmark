{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdde739-1e75-4a83-b9d5-38980de2c7c8",
   "metadata": {},
   "source": [
    "# DeepEval Framework Testing\n",
    "\n",
    "### Date: August 23, 2024\n",
    "### Author: Selin Kaplanoglu\n",
    "### Currently on pause\n",
    "**Goal:** To test DeepEval Framework to evaluate LLM responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec84c5b0-be5d-4a8b-8d12-93a938ad6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.metrics import (HallucinationMetric, \n",
    "                              FaithfulnessMetric, \n",
    "                              BiasMetric,\n",
    "                              ToolCorrectnessMetric\n",
    "                              )\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd4ed4-242f-4345-9580-575d8dccc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom llm class for deepeval\n",
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_path: str = None):\n",
    "\n",
    "        # Define quantization configuration for 4-bit model\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        # Load the 4-bit model and tokenizer\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "\n",
    "        # Define text generation pipeline\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2500,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Create parser required for JSON confinement using lmformatenforcer\n",
    "        parser = JsonSchemaParser(schema.model_json_schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
    "            pipeline.tokenizer, parser\n",
    "        )\n",
    "\n",
    "        # Generate text output\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "        # Remove special tokens and parse JSON\n",
    "        output = output.replace('\\n', ' ').strip()\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        name = self.model_name\n",
    "        return ' '.join(name.split('-'))\n",
    "\n",
    "# Define schema for the BaseModel\n",
    "class Schema(BaseModel):\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46493e-529a-4b3a-af9d-41169e2b338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model path for the CustomLlama3_8B model\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Initialize the CustomLlama3_8B model with the specified model path\n",
    "llama = CustomLlama3_8B(model_path=model_path)\n",
    "# Define the schema for the BaseModel\n",
    "schema = Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input prompt\n",
    "input = 'You are a Scientist. Tell me about TAF13.'\n",
    "\n",
    "# Generate text based on input using the llama model and specified schema\n",
    "actual_output = llama.generate(input, schema)\n",
    "\n",
    "# Print the actual output\n",
    "print(actual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804c08-682b-437d-8148-329a74c1c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define context\n",
    "context = [\"TAF13, or TATA-Box Binding Protein Associated Factor 13, is a protein that is encoded by the TAF13 gene in humans.\",\n",
    "           \"It is a subunit of the transcription initiation factor TFIID\",\n",
    "           \"TAF13 is involved in RNA polymerase II transcription initiation and promoter clearance: TAF13 is part of the TFIID complex,which plays a major role in the initiation of transcription that is dependent on RNA polymerase II.\",\n",
    "           \"TAF13 is involved in gene expression.\",\n",
    "           \"TAF13 is involved in DNA-binding transcription factor activity.\"]\n",
    "\n",
    "# Define a test case with input, actual output, context, and retrieval context\n",
    "test_case = LLMTestCase(\n",
    "    input=input,\n",
    "    actual_output= actual_output.answer,  # EXTRACT STRING WITH .answer\n",
    "    context=context,\n",
    "    retrieval_context=[\"transcription initiation factor\"],\n",
    ")\n",
    "\n",
    "# Initialize metrics for evaluation\n",
    "hallucination_metric = HallucinationMetric(model=llama)\n",
    "faithfulness_metric = FaithfulnessMetric(model=llama)\n",
    "bias_metric = BiasMetric(model=llama)\n",
    "\n",
    "# Evaluate test case using defined metrics\n",
    "# Or evaluate test cases in bulk\n",
    "# evaluate([test_case], [hallucination_metric, faithfulness_metric, bias_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval",
   "language": "python",
   "name": "deepeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdde739-1e75-4a83-b9d5-38980de2c7c8",
   "metadata": {},
   "source": [
    "# DeepEval Framework Testing\n",
    "\n",
    "### Date: 23 August, 2024\n",
    "### Currently on pause\n",
    "### Goal: To test DeepEval Framework to evaluate LLM responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec84c5b0-be5d-4a8b-8d12-93a938ad6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval imports\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.integrations.llama_index import DeepEvalAnswerRelevancyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97aadd27-9a52-4ff9-be50-98aa66029ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DGX01/Personal/slndir/deepeval/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#custom LLM setup\n",
    "import torch\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd4ed4-242f-4345-9580-575d8dccc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlama3_70B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model_kwargs = {\"torch_dtype\": torch.bfloat16}\n",
    "\n",
    "        # Load the model with the appropriate device map\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            **self.model_kwargs\n",
    "        )\n",
    "\n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2500,\n",
    "            truncation=True,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Create parser required for JSON confinement using lmformatenforcer\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
    "            pipeline.tokenizer, parser\n",
    "        )\n",
    "\n",
    "        # Generate output\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "        # Clean up the output to ensure valid JSON\n",
    "        cleaned_output = self._clean_json_output(output)\n",
    "\n",
    "        # Try to load the JSON\n",
    "        try:\n",
    "            json_result = json.loads(cleaned_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Failed to parse JSON: {e}.\\nGenerated output: {cleaned_output}\")\n",
    "\n",
    "        # Return valid JSON object according to the schema DeepEval supplied\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def _clean_json_output(self, output: str) -> str:\n",
    "        # Attempt to clean up the output by ensuring it has balanced braces and is properly closed\n",
    "        output = output.strip()\n",
    "\n",
    "        # Check if the output starts with a '{' and ends with a '}', if not try to fix it\n",
    "        if not output.startswith(\"{\"):\n",
    "            output = \"{\" + output\n",
    "        if not output.endswith(\"}\"):\n",
    "            output += \"}\"\n",
    "\n",
    "        # Remove any characters after the closing brace\n",
    "        closing_brace_index = output.rfind(\"}\")\n",
    "        if closing_brace_index != -1:\n",
    "            output = output[:closing_brace_index + 1]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3.1 70B Instruct\"\n",
    "\n",
    "# Define a schema using Pydantic\n",
    "class Schema(BaseModel):\n",
    "   answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c46493e-529a-4b3a-af9d-41169e2b338b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae483bce855842768c62a6a494b10c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eda9dddf38b459694b61ba2be66ea67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Strawberries are a type of fruit that belongs to the rose family. \\nStrawberries are native to the continent of Antarctica. \\n\\nWhich one is the truth and which one is the lie?'\n"
     ]
    }
   ],
   "source": [
    "custom_llm = CustomLlama3_70B()\n",
    "user_input = \"give me one truth about strawberries. then give me one lie about strawberries\"\n",
    "\n",
    "# Call the generate method with the schema\n",
    "llm_output = custom_llm.generate(user_input, schema=Schema)\n",
    "\n",
    "# Print the output as JSON\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7b1e37-bb3d-454f-aeba-7023427e850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the actual output contains information that is not present in the context, specifically the claim that strawberries are native to Antarctica, which contradicts the provided context that states they are native to regions in the Northern Hemisphere, particularly in Europe, North America, and Asia, not Antarctica. This indicates a complete fabrication of information by the model, hence the perfect hallucination score of 1.00\n"
     ]
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=user_input,\n",
    "    actual_output=llm_output,\n",
    "    context=[\"\"\"Strawberries are a type of fruit that belongs to the rose family. Strawberries are native to regions in the Northern Hemisphere, particularly in Europe, North America, and Asia.\"\"\"],\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(threshold=0.5, model=custom_llm)\n",
    "metric.measure(test_case)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804c08-682b-437d-8148-329a74c1c623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval",
   "language": "python",
   "name": "deepeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

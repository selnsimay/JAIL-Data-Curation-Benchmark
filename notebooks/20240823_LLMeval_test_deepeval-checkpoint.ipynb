{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdde739-1e75-4a83-b9d5-38980de2c7c8",
   "metadata": {},
   "source": [
    "# DeepEval Framework Testing\n",
    "\n",
    "### Date: August 23, 2024\n",
    "### Author: Selin Kaplanoglu\n",
    "### Currently on pause\n",
    "**Goal:** To test DeepEval Framework to evaluate LLM responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec84c5b0-be5d-4a8b-8d12-93a938ad6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.metrics import (HallucinationMetric, \n",
    "                              FaithfulnessMetric, \n",
    "                              BiasMetric,\n",
    "                              ToolCorrectnessMetric\n",
    "                              )\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd4ed4-242f-4345-9580-575d8dccc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom llm class for deepeval\n",
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_path: str = None):\n",
    "\n",
    "        # Define quantization configuration for 4-bit model\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        # Load the 4-bit model and tokenizer\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "\n",
    "        # Define text generation pipeline\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2500,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Create parser required for JSON confinement using lmformatenforcer\n",
    "        parser = JsonSchemaParser(schema.model_json_schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
    "            pipeline.tokenizer, parser\n",
    "        )\n",
    "\n",
    "        # Generate text output\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "        # Remove special tokens and parse JSON\n",
    "        output = output.replace('\\n', ' ').strip()\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        name = self.model_name\n",
    "        return ' '.join(name.split('-'))\n",
    "\n",
    "# Define schema for the BaseModel\n",
    "class Schema(BaseModel):\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c46493e-529a-4b3a-af9d-41169e2b338b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae483bce855842768c62a6a494b10c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eda9dddf38b459694b61ba2be66ea67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Strawberries are a type of fruit that belongs to the rose family. \\nStrawberries are native to the continent of Antarctica. \\n\\nWhich one is the truth and which one is the lie?'\n"
     ]
    }
   ],
   "source": [
    "# Define the model path for the CustomLlama3_8B model\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Initialize the CustomLlama3_8B model with the specified model path\n",
    "llama = CustomLlama3_8B(model_path=model_path)\n",
    "# Define the schema for the BaseModel\n",
    "schema = Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input prompt\n",
    "input = 'You are a Scientist. Tell me about TAF13.'\n",
    "\n",
    "# Generate text based on input using the llama model and specified schema\n",
    "actual_output = llama.generate(input, schema)\n",
    "\n",
    "# Print the actual output\n",
    "print(actual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7b1e37-bb3d-454f-aeba-7023427e850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the actual output contains information that is not present in the context, specifically the claim that strawberries are native to Antarctica, which contradicts the provided context that states they are native to regions in the Northern Hemisphere, particularly in Europe, North America, and Asia, not Antarctica. This indicates a complete fabrication of information by the model, hence the perfect hallucination score of 1.00\n"
     ]
    }
   ],
   "source": [
    "# Define context\n",
    "context = [\"TAF13, or TATA-Box Binding Protein Associated Factor 13, is a protein that is encoded by the TAF13 gene in humans.\",\n",
    "           \"It is a subunit of the transcription initiation factor TFIID\",\n",
    "           \"TAF13 is involved in RNA polymerase II transcription initiation and promoter clearance: TAF13 is part of the TFIID complex,which plays a major role in the initiation of transcription that is dependent on RNA polymerase II.\",\n",
    "           \"TAF13 is involved in gene expression.\",\n",
    "           \"TAF13 is involved in DNA-binding transcription factor activity.\"]\n",
    "\n",
    "# Define a test case with input, actual output, context, and retrieval context\n",
    "test_case = LLMTestCase(\n",
    "    input=input,\n",
    "    actual_output= actual_output.answer,  # EXTRACT STRING WITH .answer\n",
    "    context=context,\n",
    "    retrieval_context=[\"transcription initiation factor\"],\n",
    ")\n",
    "\n",
    "# Initialize metrics for evaluation\n",
    "hallucination_metric = HallucinationMetric(model=llama)\n",
    "faithfulness_metric = FaithfulnessMetric(model=llama)\n",
    "bias_metric = BiasMetric(model=llama)\n",
    "\n",
    "# Evaluate test case using defined metrics\n",
    "# Or evaluate test cases in bulk\n",
    "# evaluate([test_case], [hallucination_metric, faithfulness_metric, bias_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804c08-682b-437d-8148-329a74c1c623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval",
   "language": "python",
   "name": "deepeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
